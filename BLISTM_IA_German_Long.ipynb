{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BLISTM_IA_German_Long.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"zxM6GWAqyj8f"},"source":["import numpy as np\n","import pandas as pd\n","from collections import Counter\n","import string\n","import  random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim \n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchtext import data\n","from sklearn.metrics import mean_absolute_error as MAE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-JQItyOmKm8s","executionInfo":{"status":"ok","timestamp":1609435734038,"user_tz":-120,"elapsed":4930,"user":{"displayName":"Cristi Ranete","photoUrl":"","userId":"13149360052663211112"}},"outputId":"e42f7e0f-eeaf-4455-c6d5-9d1178692815"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1iVgq_LMvTi","executionInfo":{"status":"ok","timestamp":1609435734041,"user_tz":-120,"elapsed":4925,"user":{"displayName":"Cristi Ranete","photoUrl":"","userId":"13149360052663211112"}},"outputId":"f16eceb5-bc56-4ade-fa05-4cd000d39cb3"},"source":["cd drive/MyDrive/ML_German"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/ML_German\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rN4jfn3S9dYY"},"source":["use_cuda = torch.cuda.is_available()\n","torch.manual_seed(1024)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","torch.backends.cudnn.deterministic = True  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1VD3qDLdrfL"},"source":["train_data = pd.read_csv('./csv_files/training_no_emoji.csv')\r\n","validation_data = pd.read_csv('./csv_files/validation_no_emoji.csv') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cganbjxEdwT4"},"source":["allowed_charachters = [ch for ch in ' abcdefghijklmnopqrstuvwxyz' + 'öäüß']\r\n","def filter_texts(texts):\r\n","  return [''.join([ch for ch in text.lower() if ch in allowed_charachters]) for text in texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7BTFFHkdyaD"},"source":["data_train, values_train = train_data['Text'], train_data['Long']\r\n","data_valid, values_valid = validation_data['Text'], validation_data['Long']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHOgcX5dd0S_"},"source":["data_train = filter_texts(data_train)\r\n","data_valid = filter_texts(data_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLn3e1QGp66S"},"source":["# train_data = pd.read_csv('train_data.csv', sep='\\t', encoding='utf-8', lineterminator='\\n', header = 0, names=['Id', 'Text', 'Label'])\n","# validation_data = pd.read_csv('valid_data.csv', sep='\\t', encoding='utf-8', lineterminator='\\n', header = 0, names=['Id', 'Text', 'Label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ddsD3n9jkGh"},"source":["def full_texts(texts):\n","  text = \" \"\n","  for it in texts:\n","    text += \" \".join(it)\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wFonLEkkMNl"},"source":["text = full_texts(data_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l9SwFkcVjbPx"},"source":["class Vocabulary:\n","    \"\"\"\n","    Helper class that maps characters to unique indices and the other way around\n","    \"\"\"\n","    def __init__(self, text: str):\n","        # PAD is a special character for padding shorter sequences \n","        # in a mini-batch\n","        # create a set out of all characters\n","        characters_set = set([\"0\"]) \n","        characters_set.update(text)\n","        \n","        #create a dictionary for characters\n","        self.char_to_idx = {char:idx for (idx, char) \n","                            in enumerate(characters_set)}\n","        self.idx_to_char = {idx:char for (idx, char) \n","                            in enumerate(characters_set)}\n","   \n","    def size(self):\n","        return len(self.char_to_idx)\n","      \n","    def __str__(self):\n","        return str(self.char_to_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9thlSu5kcTb","executionInfo":{"status":"ok","timestamp":1609435738376,"user_tz":-120,"elapsed":9227,"user":{"displayName":"Cristi Ranete","photoUrl":"","userId":"13149360052663211112"}},"outputId":"d363460a-90f4-482d-8b34-1d6a6520a4df"},"source":["vocab = Vocabulary(text)\n","print(\"Vocabulary size: \", vocab.size())\n","print(\"Vocabulary: \\n\", vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocabulary size:  32\n","Vocabulary: \n"," {'d': 0, 't': 1, 'v': 2, 'n': 3, 'ü': 4, 'ä': 5, 'o': 6, 'e': 7, '0': 8, 'w': 9, 'j': 10, 'z': 11, 'b': 12, 'c': 13, 'u': 14, 'i': 15, 'q': 16, 'p': 17, 'ö': 18, 'l': 19, 'h': 20, 'f': 21, 'ß': 22, 'x': 23, 's': 24, 'g': 25, 'a': 26, 'k': 27, 'r': 28, 'y': 29, 'm': 30, ' ': 31}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_CBJbAqfiSGv"},"source":["def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n","    \"\"\"\n","    Convert a string to a Tensor with corresponding character indices\n","    e.g. \"We have\" -> [48, 13,  2, 66, 56, 31, 13 \n","    \"\"\"\n","    text_indices = [vocab.char_to_idx[c] for c in text]\n","  \n","    return torch.tensor(text_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-n17Bx0ReBRc"},"source":["# function that prepers bacthes \n","def my_collate(batch):\n","  sizes = []\n","  for item in batch:\n","    sizes.append(torch.tensor(len(item[0])))\n","  sizes = torch.stack(sizes, dim = 0).long()\n","  max_size = torch.max(sizes, dim = 0)[0]\n","  new_data = []\n","  for item in batch:\n","    new_data.append(F.pad(input=item[0], pad=(0, 500 - item[0].shape[0]), mode='constant', value=vocab.char_to_idx['0']))\n","  data = torch.stack(new_data, dim = 0)\n","  target = torch.stack([torch.tensor(item[1]) for item in batch], dim = 0)\n","  return [data, sizes, target]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SnzscKN3luU2"},"source":["class TextsDataset(Dataset):\n","    def __init__(self, texts, labels=None, vocab = None, max_length = 500):\n","        self.X = texts\n","        self.y = labels\n","        self.vocab = vocab\n","        self.max_len = max_length\n","         \n","    def __len__(self):\n","        return (len(self.X))\n","    \n","    def __getitem__(self, i):\n","        data = self.X[i]\n","        data = text_to_tensor(data, self.vocab)\n","        if self.y is not None:\n","            y = self.y[i]\n","            return (data, y)\n","        else:\n","            return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mpxPz5npaem"},"source":["training_dataset = TextsDataset(data_train, values_train, vocab, 500)\n","validing_dataset = TextsDataset(data_valid, values_valid, vocab, 500)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jtRWzDiwhQu"},"source":["batch_size = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EzhzpGVqo-_"},"source":["trainloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last = True, collate_fn=my_collate)\n","validloader = DataLoader(validing_dataset, batch_size=batch_size, shuffle=True, drop_last = True, collate_fn=my_collate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHw8VDAD_bay"},"source":["class BiLSTM(nn.Module):\n","    \n","    def __init__(self, vocab_size, output_size, embed_size, hidden_nodes, n_layers, drop_prob=0.5):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_nodes, bidirectional=True, batch_first=True, num_layers = n_layers)\n","        self.linear = nn.Linear(hidden_nodes*4 , 64)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(drop_prob)\n","        self.out = nn.Linear(64, output_size)\n","\n","\n","    def forward(self, x, text_sizes):\n","        h_embedding = self.embedding(x)\n","        #packed_embedded = nn.utils.rnn.pack_padded_sequence(h_embedding, text_sizes.flatten(), batch_first=True, enforce_sorted=False)\n","        h_lstm, _ = self.lstm(h_embedding)\n","        #h_lstm = nn.utils.rnn.pad_packed_sequence(h_lstm, True)[0]\n","        avg_pool = torch.mean(h_lstm, 1)\n","        max_pool, _ = torch.max(h_lstm, 1)\n","        conc = torch.cat(( avg_pool, max_pool), 1)\n","        conc = self.relu(self.linear(conc))\n","        conc = self.dropout(conc)\n","        out = self.out(conc)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DfrrGOXAeif"},"source":["size_of_vocab = vocab.size()\n","embedding_dim = 128\n","num_hidden_nodes = 64\n","num_output_nodes = 1\n","num_layers = 2\n","dropout = 0.2\n","\n","#instantiate the model\n","model_r = BiLSTM(size_of_vocab, num_output_nodes, embedding_dim, num_hidden_nodes, num_layers, dropout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSXT8u52A4SD","executionInfo":{"status":"ok","timestamp":1609435738383,"user_tz":-120,"elapsed":9203,"user":{"displayName":"Cristi Ranete","photoUrl":"","userId":"13149360052663211112"}},"outputId":"5285d53c-4183-4652-b875-2f93b4a33732"},"source":["#architecture\n","print(model_r)\n","\n","#No. of trianable parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model_r.parameters() if p.requires_grad)\n","    \n","print(f'The model has {count_parameters(model_r):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BiLSTM(\n","  (embedding): Embedding(32, 128)\n","  (lstm): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=256, out_features=64, bias=True)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (out): Linear(in_features=64, out_features=1, bias=True)\n",")\n","The model has 219,265 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WHpaoKSpBNub"},"source":["#define optimizer and loss\n","optimizer = optim.Adam(model_r.parameters(), lr = 0.005)\n","criterion = nn.L1Loss()\n","    \n","#push to cuda if available\n","model_r = model_r.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZDwmqRlBbYS"},"source":["def train(model, train_iterator, optimizer, criterion):\n","    \n","    #initialize every epoch \n","    epoch_loss = 0\n","    clip = 3\n","    \n","    #set the model in training phase\n","    model.train()  \n","    for inputs, text_lengths, labels in train_iterator:\n","        #print(it)\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        #resets the gradients after every batch\n","        optimizer.zero_grad()   \n","        \n","        #retrieve text and no. of words\n","        #print(\"retrive\")\n","        output = model(inputs, text_lengths) \n","        \n","        #compute the loss\n","        #print(\"loss\")\n","        loss = criterion(output.squeeze(), labels.float())  \n","        \n","        #print(\"back\")\n","        #backpropage the loss and compute the gradients\n","        loss.backward() \n","        nn.utils.clip_grad_norm_(model.parameters(), clip)      \n","        \n","        #print(\"optim\")\n","        #update the weights\n","        optimizer.step()      \n","        \n","        #loss and accuracy\n","        epoch_loss += loss.item()  \n","\n","    return epoch_loss / len(train_iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Nviwv9XB2Vr"},"source":["def evaluate(model, eval_iterator, criterion):\n","    \n","    #initialize every epoch\n","    epoch_loss = 0\n","    clip = 5\n","\n","    #deactivating dropout layers\n","    model.eval()\n","    \n","    #deactivates autograd\n","    with torch.no_grad():\n","    \n","        for inputs, text_lengths, labels in eval_iterator:\n","\n","            #retrieve text and no. of words\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            \n","            #convert to 1d tensor\n","            output = model(inputs, text_lengths)\n","            \n","            #compute loss and accuracy\n","            test_loss = criterion(output.squeeze(), labels.float())\n","            \n","            #keep track of loss and accuracy\n","            epoch_loss += test_loss.item()\n","        \n","    return epoch_loss / len(eval_iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-KK8uXPICuh","executionInfo":{"status":"ok","timestamp":1609435739361,"user_tz":-120,"elapsed":10166,"user":{"displayName":"Cristi Ranete","photoUrl":"","userId":"13149360052663211112"}},"outputId":"02b5cc6f-afb9-48a0-db5e-7ecc56810fac"},"source":["model_r.load_state_dict(torch.load(\"saved_weights_1.pt\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54Th5uzGB94j","outputId":"5f1e4b47-ceb8-4a73-9efd-0699ca22cbc9"},"source":["N_EPOCHS = 50\n","best_valid_loss = 0.654\n","\n","for epoch in range(N_EPOCHS):\n","     \n","    print(\"Epoch: \", epoch)\n","    #train the model\n","    train_loss = train(model_r, trainloader, optimizer, criterion)\n","    \n","    #evaluate the model\n","    valid_loss = evaluate(model_r, validloader, criterion)\n","     \n","   #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model_r.state_dict(), 'saved_weights_best_valid_long.pt')\n","    torch.save(model_r.state_dict(), 'saved_weights_long.pt') \n","    print(f'\\tTrain Loss: {train_loss:.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch:  0\n","\tTrain Loss: 0.966\n","\t Val. Loss: 0.779\n","Epoch:  1\n","\tTrain Loss: 0.804\n","\t Val. Loss: 0.739\n","Epoch:  2\n","\tTrain Loss: 0.769\n","\t Val. Loss: 0.717\n","Epoch:  3\n","\tTrain Loss: 0.734\n","\t Val. Loss: 0.699\n","Epoch:  4\n","\tTrain Loss: 0.710\n","\t Val. Loss: 0.686\n","Epoch:  5\n","\tTrain Loss: 0.688\n","\t Val. Loss: 0.680\n","Epoch:  6\n","\tTrain Loss: 0.669\n","\t Val. Loss: 0.683\n","Epoch:  7\n","\tTrain Loss: 0.656\n","\t Val. Loss: 0.663\n","Epoch:  8\n","\tTrain Loss: 0.645\n","\t Val. Loss: 0.678\n","Epoch:  9\n","\tTrain Loss: 0.632\n","\t Val. Loss: 0.677\n","Epoch:  10\n","\tTrain Loss: 0.618\n","\t Val. Loss: 0.667\n","Epoch:  11\n","\tTrain Loss: 0.608\n","\t Val. Loss: 0.690\n","Epoch:  12\n","\tTrain Loss: 0.596\n","\t Val. Loss: 0.669\n","Epoch:  13\n","\tTrain Loss: 0.597\n","\t Val. Loss: 0.657\n","Epoch:  14\n","\tTrain Loss: 0.590\n","\t Val. Loss: 0.680\n","Epoch:  15\n","\tTrain Loss: 0.584\n","\t Val. Loss: 0.691\n","Epoch:  16\n","\tTrain Loss: 0.572\n","\t Val. Loss: 0.679\n","Epoch:  17\n","\tTrain Loss: 0.567\n","\t Val. Loss: 0.654\n","Epoch:  18\n","\tTrain Loss: 0.560\n","\t Val. Loss: 0.708\n","Epoch:  19\n","\tTrain Loss: 0.548\n","\t Val. Loss: 0.668\n","Epoch:  20\n","\tTrain Loss: 0.536\n","\t Val. Loss: 0.667\n","Epoch:  21\n","\tTrain Loss: 0.537\n","\t Val. Loss: 0.667\n","Epoch:  22\n","\tTrain Loss: 0.528\n","\t Val. Loss: 0.746\n","Epoch:  23\n","\tTrain Loss: 0.521\n","\t Val. Loss: 0.666\n","Epoch:  24\n"],"name":"stdout"}]}]}